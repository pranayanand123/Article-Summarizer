{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pranay\n",
      "[nltk_data]     Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Pranay\n",
      "[nltk_data]     Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_words = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency(word_sent):\n",
    "    max_value = 0.9\n",
    "    min_value = 0.1\n",
    "    frequencies = defaultdict(int)\n",
    "    for s in word_sent:\n",
    "        for w in s:\n",
    "            if w not in stopping_words:\n",
    "                frequencies[w]+=1\n",
    "    \n",
    "    max_freq  = float(max(frequencies.values()))\n",
    "    for w in list(frequencies):\n",
    "        frequencies[w] = frequencies[w]/max_freq\n",
    "        if frequencies[w]>=max_value or frequencies[w]<=min_value:\n",
    "            del frequencies[w]\n",
    "    return frequencies     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,n): \n",
    "    sents = sent_tokenize(text) \n",
    "    #assert n<=len(sents) \n",
    "    word_sent = [word_tokenize(s.lower()) for s in sents] \n",
    "    freq= compute_frequency(word_sent) \n",
    "    rank = defaultdict(int) \n",
    "    for i,sent in enumerate(word_sent): \n",
    "        for w in sent: \n",
    "            if w in freq: \n",
    "                rank[i] += freq[w] \n",
    "    sent_index= sorted(rank, key=rank.get, reverse=True)[:n] \n",
    "    return [sents[j] for j in sent_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    page = urllib.request.urlopen(url).read().decode('utf8')\n",
    "    \"\"\"soup = BeautifulSoup(page, \"html5lib\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\"\"\"\n",
    "    soup2 = BeautifulSoup(page, \"html5lib\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "    return soup2.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[ Why Facebook users’ data obtained by Cambridge Analytica has probably spun far out of reach ] To be clear, under this model your data isn't stored directly on the blockchain; the key you provide simply points to the place on your hard drive or server where you've stored your data.\",\n",
       " \"“If that system is built and the technology is developed, some of\\xa0the issues\\xa0we’ve seen related to data breaches, access to people’s personal info\\xa0— that would potentially improve,” said Aaron Wright, founder of the Blockchain Project at Yeshiva University's Cardozo School of Law.\",\n",
       " \"It's been a terrible week for Facebook, with policymakers and users alike\\xa0demanding answers from\\xa0the social network over its Cambridge Analytica fiasco, in which\\xa0the data analysis firm improperly accessed the personal information of about 50 million Facebook users.\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.washingtonpost.com/news/the-switch/wp/2018/03/23/the-new-technology-that-aspires-to-deletefacebook-for-good/?utm_term=.b97465872ca3\"\n",
    "text  = get_text(url)\n",
    "summary = summarize(text[1],3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
